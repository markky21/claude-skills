---
description: Iteratively validate and fix code issues until no critical/high findings remain
allowed-tools: Bash(git *), Task, AskUserQuestion, TodoWrite, Read, Edit, Glob, Grep
---

# Fix Loop Command

Iteratively validates code and fixes issues until no critical/high severity findings remain (inspired by the [Ralph Wiggum technique](https://awesomeclaude.ai/ralph-wiggum)).

## Context

- Current branch: !`git branch --show-current`
- Base branch: !`git merge-base HEAD main 2>/dev/null && echo "main" || echo "HEAD~5"`
- Files changed: !`git diff main...HEAD --name-only 2>/dev/null || git diff HEAD~5...HEAD --name-only`
- File count: !`git diff main...HEAD --name-only 2>/dev/null | wc -l || git diff HEAD~5...HEAD --name-only | wc -l`

## Your Task

Execute an iterative fix loop following this process:

### Step 1: Discover Available Validators

Use Task tool to launch fix-loop-validator-discovery agent to find all available validators:

```
Pass to the agent:
- BUILTIN_MANIFEST: plugins/my-personal-tools/validators.json
- PLUGINS_DIR: <current project's installed plugins directory>
- PROJECT_DIR: <current project root>
```

The discovery agent will:
- Load built-in validators from validators.json
- Scan external plugins for validators in plugin.json
- Use fallback naming convention for *-validator agents
- Return a JSON list of all discovered validators with metadata

Capture the JSON output from the discovery agent and store it.

### Step 2: Parse CLI Arguments & Run Configurator

Check if CLI arguments were provided. Arguments format:
```
--validators ddd-oop,dry,react-best-practices
--severity critical,high
--iterations 5
```

**If CLI arguments provided:**
- Parse them (split by commas, convert to proper format)
- Skip to Step 2b (validation section)

**If NO CLI arguments provided:**
- Use Task tool to launch fix-loop-configurator agent
- Pass the discovered validators list from Step 1
- The configurator will invoke AskUserQuestion for three interactive prompts:
  1. Validator selection (multi-select)
  2. Severity filter (multi-select)
  3. Iteration count (single-select)
- Capture the configuration JSON output

### Step 2b: Validate & Merge Configuration

- Extract selectedValidators array from configurator output
- Validate that all selected validators were discovered in Step 1
- Warn if a validator is selected but not found: "âš ï¸ Validator X not found during discovery, skipping"
- Build final selectedValidators list with agent names from discovered metadata
- Store: severity levels, maxIterations
- Confirm configuration is ready to proceed

### Step 2c: Extract Test Configuration

From the configurator output (or defaults if skipped), extract:
- `generateTests` - boolean, true if user enabled test generation
- `testScope` - "all-changes" | "business-logic" | "flagged-code"
- `testCoverage` - "happy-paths-edges" | "happy-paths-only" | "comprehensive"

If user disabled test generation, set `generateTests = false` and skip all subsequent test steps.

If test generation enabled, proceed with test steps in the loop.

### Step 3: Initialize Loop State

Create a todo list to track the loop:

```
- [ ] Iteration 1: Validate and fix
- [ ] Iteration 2: Validate and fix (if needed)
- [ ] ... up to max iterations
- [ ] Generate final summary
```

Initialize variables:
- `iteration = 1`
- `totalFixed = 0`
- `previousIssueCount = Infinity`
- `stallCount = 0`

### Step 4: Run Iteration Loop

For each iteration until termination:

#### 4a. Display Iteration Header

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 ğŸ”„ Iteration {n}/{max}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ Running validators: {selected validators}
   Scope: {file count} files changed vs main
```

#### 4a.1: Log Selected Validators

Display which validators will run:

```
ğŸ“‹ Running validation loop with:
   Validators: {count} selected
     {validator name 1}
     {validator name 2}
   Severity: {CRITICAL, HIGH, etc.}
   Max iterations: {number}
```

#### 4b. Run Validators in Parallel

Use Task tool to launch selected validator agents **in parallel**:

**DDD/OOP Validator prompt:**
```
Validate the branch diff for DDD/OOP compliance.

**CRITICAL SCOPE CONSTRAINT:**
- ONLY report findings on lines that appear in the git diff (lines starting with `+`)
- If an issue exists in a file but the code was NOT modified in this branch, do NOT report it
- The goal is to validate the CHANGES, not the entire codebase
- Before reporting any finding, verify the problematic code appears in the diff

Changed files:
{list of changed files}

Git diff to analyze:
{paste actual git diff main...HEAD output here}

Check for:
- Anemic domain models (data without behavior)
- Tell/Don't Ask violations (extracting data to decide externally)
- Method placement (logic in wrong object/service)
- Parameter bloat (too many primitives)

Output findings with:
- Severity: ğŸ”´ CRITICAL / ğŸŸ  HIGH / ğŸŸ¡ MEDIUM / ğŸŸ¢ LOW
- Location: ğŸ“ file:line
- Recommendation: ğŸ’¡ how to fix
```

**DRY Validator prompt:**
```
Validate the branch diff for DRY violations.

**CRITICAL SCOPE CONSTRAINT:**
- ONLY report findings on lines that appear in the git diff (lines starting with `+`)
- If an issue exists in a file but the code was NOT modified in this branch, do NOT report it
- The goal is to validate the CHANGES, not the entire codebase
- Before reporting any finding, verify the problematic code appears in the diff

Changed files:
{list of changed files}

Git diff to analyze:
{paste actual git diff main...HEAD output here}

Check for:
- Duplicated constants/enums
- Repeated logic patterns
- Magic strings/numbers
- Similar code that should be unified

Output findings with:
- Severity: ğŸ”´ CRITICAL / ğŸŸ  HIGH / ğŸŸ¡ MEDIUM / ğŸŸ¢ LOW
- Location: ğŸ“ file:line (for each occurrence)
- Recommendation: ğŸ’¡ how to fix
```

**Clean Code Validator prompt:**
```
Validate the branch diff for clean code principles.

**CRITICAL SCOPE CONSTRAINT:**
- ONLY report findings on lines that appear in the git diff (lines starting with `+`)
- If an issue exists in a file but the code was NOT modified in this branch, do NOT report it
- The goal is to validate the CHANGES, not the entire codebase
- Before reporting any finding, verify the problematic code appears in the diff

Changed files:
{list of changed files}

Git diff to analyze:
{paste actual git diff main...HEAD output here}

Check for:
- Long functions (>20 lines)
- Poor naming (unclear intent)
- SOLID violations
- Code smells (feature envy, god class, etc.)

Output findings with:
- Severity: ğŸ”´ CRITICAL / ğŸŸ  HIGH / ğŸŸ¡ MEDIUM / ğŸŸ¢ LOW
- Location: ğŸ“ file:line
- Recommendation: ğŸ’¡ how to fix
```

#### 4b.1: Validate & Parse Validator Output

For each validator that completed:

1. **Check output format compliance**
   - Use fix-loop-output-validator agent to validate the actual output
   - Pass the validator's complete output text
   - Receive JSON validation result

2. **Log validation result**
   - If valid: log "âœ… {validator_name}: output format valid"
   - If invalid: log "âš ï¸ {validator_name}: output format non-compliant, attempting to parse anyway"

3. **Proceed to parsing**
   - Both valid and invalid outputs proceed to Step 4c for parsing
   - Invalid outputs may have some findings skipped if unparseable
   - Non-blocking: one validator's format issues don't stop others

Example:
```
Validating outputs:
  âœ… ddd-oop-validator: output format valid
  âœ… dry-violations-detector: output format valid
  âš ï¸ react-best-practices-validator: output format non-compliant
     (Parsing anyway - some findings may be skipped)
```

#### 4c. Filter and Count Findings

From all validator outputs, extract findings matching selected severity levels.

Parse findings using standardized regex patterns that work for ANY validator:

**Severity emoji + description regex:**
```
/(ğŸ”´|ğŸŸ |ğŸŸ¡|ğŸŸ¢)\s+([A-Z]+):\s+(.+)/
```
Extracts: emoji, severity level (CRITICAL/HIGH/MEDIUM/LOW), description

**Location regex:**
```
/ğŸ“\s+(.+):(\d+)/
```
Extracts: file path and line number

**Recommendation regex:**
```
/ğŸ’¡\s+(.+)/
```
Extracts: recommendation text

**Processing steps:**

For each validator output line:
1. Try to match against severity regex
2. If matches: extract emoji and map to severity level
   - ğŸ”´ â†’ CRITICAL
   - ğŸŸ  â†’ HIGH
   - ğŸŸ¡ â†’ MEDIUM
   - ğŸŸ¢ â†’ LOW
3. Check if severity level is in selectedSeverity
4. If yes: scan next lines to build complete finding
   - Scan forward from current line until:
     - You find a line with ğŸ“ pattern (location)
     - You find a line with ğŸ’¡ pattern (recommendation)
     - OR until the next severity emoji (ğŸ”´/ğŸŸ /ğŸŸ¡/ğŸŸ¢) appears
     - OR until end of validator output
   - If BOTH ğŸ“ and ğŸ’¡ patterns are found: create complete finding object
   - If EITHER pattern is missing: log warning and SKIP this finding
     ```
     âš ï¸ INCOMPLETE: Finding missing required field in {validator_name}:
        {brief description}
        (Skipping - location and recommendation required)
     ```
5. Combine into finding only if COMPLETE: {severity, location, description, recommendation}
   - All four fields required
   - Add to findings list ONLY if complete
   - Incomplete findings logged and skipped

**Error handling:**

If parsing fails for any line:
```
âš ï¸ WARNING: Could not parse finding from {validator_name}:
   {line_that_failed}
   (Skipping this finding but continuing with others)
```

Continue without stopping - prioritize fix-loop robustness.

**Counting:**

Count all successfully parsed findings that match selectedSeverity:
```
currentIssueCount = number of matching findings
```

Log the count with breakdown:
```
Found {currentIssueCount} issues:
  ğŸ”´ CRITICAL: {count}
  ğŸŸ  HIGH: {count}
  ğŸŸ¡ MEDIUM: {count}
  ğŸŸ¢ LOW: {count}
```

#### 4d. Check Termination Conditions

**Condition 1: Success**
If `currentIssueCount == 0`:
```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 âœ… No {severity levels} issues found!
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```
â†’ Go to Step 4 (Summary)

**Condition 2: Max Iterations**
If `iteration >= maxIterations`:
```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 â¹ï¸ Max iterations reached. {currentIssueCount} issues remaining.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```
â†’ Go to Step 4 (Summary)

**Condition 3: Stall Detection**
If `currentIssueCount >= previousIssueCount`:
  `stallCount++`
  If `stallCount >= 2`:
```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 âš ï¸ No progress for 2 iterations. Stopping to prevent infinite loop.
    {currentIssueCount} issues may require manual intervention.
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```
â†’ Go to Step 4 (Summary)

Otherwise, reset `stallCount = 0`.

#### 4e. Display Findings

```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 Found {count} issues ({breakdown by severity})
 From validators: {comma-separated list of validators that found issues}
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

{For each finding, display in this format:}

{severity_emoji} {SEVERITY}: {description}
   ğŸ“ {validator_name}: {file}:{line}
   ğŸ’¡ {recommendation}

{blank line between findings}

{Example output:}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 Found 8 issues (1 CRITICAL, 3 HIGH, 3 MEDIUM, 1 LOW)
 From validators: ddd-oop-validator, react-nextjs-validator
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ”´ CRITICAL: Hook called conditionally inside if statement
   ğŸ“ react-nextjs-validator: src/components/Form.tsx:42
   ğŸ’¡ Move useState call outside the conditional block. Hooks must be called unconditionally on every render.

ğŸŸ  HIGH: Multiple useState calls managing related state
   ğŸ“ react-nextjs-validator: src/hooks/useFormState.ts:15
   ğŸ’¡ Consider using useReducer instead of 5 related useState calls for better state management.

ğŸŸ  HIGH: Anemic domain model
   ğŸ“ ddd-oop-validator: src/domain/User.ts:8
   ğŸ’¡ Add behavior methods to the User entity instead of keeping it data-only.

ğŸŸ¡ MEDIUM: Derived state stored in component state
   ğŸ“ react-nextjs-validator: src/components/UserProfile.tsx:28
   ğŸ’¡ Calculate filteredUsers directly instead of storing in state. Use useMemo if computation is expensive.

[... more findings ...]
```

#### 4f. Apply Fixes

```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 ğŸ”§ Fixing {count} issues...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

Use Task tool to launch fix-loop-fixer agent:

```
Apply fixes for the following validation findings:

{paste all findings with severity, location, recommendation}

Read each file, apply the recommended fix, and report what was changed.

Output format for each fix:
âœ… Fixed: [description]
   ğŸ“ file:line
   ğŸ“ [explanation]

Or if skipped:
âš ï¸ Skipped: [reason]
```

Track: `totalFixed += number of fixes applied`

#### 4g. Generate Tests (if enabled)

If `generateTests == false`, skip to 4h.

Otherwise:

```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 ğŸ§ª Generating tests for fixes...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

Use Task tool to launch fix-loop-test-generator agent:

**Inputs:**
- Git diff of changes (use: `git diff HEAD~{iteration}...HEAD`)
- Validator context (from previous validator runs, what each fix was about)
- Project patterns (detected from testing-patterns.json analysis)
- Test scope and coverage (from configuration)

**Output expected:**
- List of test files generated
- Count of test cases per file
- Skipped code sections with reasons

Example:

```
âœ… Generated: src/domain/User.test.ts
   ğŸ“ Tests User entity creation, email validation, domain behavior
   ğŸ“Š 6 test cases covering constructor, validation, error paths

âœ… Generated: src/services/UserRegistrationService.test.ts
   ğŸ“ Tests registration service with repository integration
   ğŸ“Š 8 test cases covering happy path, errors, state verification
```

#### 4h. Run Generated Tests (if enabled)

If `generateTests == false`, skip to 4i (continue loop).

Otherwise:

```
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 ğŸƒ Running tests...
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

Use Task tool to launch fix-loop-test-runner agent.

**Inputs:**
- Test framework type (auto-detect or from testing-patterns.json)
- Test file patterns to run

**Output expected:**
- Test results: pass/fail count
- Failed test names and error details
- Stack traces if available

Example output:

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 ğŸ§ª Test Results
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Total: 14 | âœ… Passed: 12 | âŒ Failed: 2

âŒ User should throw error for invalid email
   AssertionError: expected no error to be thrown
   ğŸ“ src/domain/User.test.ts:52

âŒ UserRegistrationService should reject duplicate users
   Error: repository.save not called
   ğŸ“ src/services/UserRegistrationService.test.ts:28
```

#### 4i. Check Test Status (if enabled)

If `generateTests == false`, go to 4j.

If all tests passed:
```
âœ… All tests passed! Fixes are validated.
```
â†’ Proceed to Step 5 (Summary) - fix-loop is done

If some tests failed:
```
âŒ {n} test failures detected. Analyzing...
```

Use Task tool to launch fix-loop-failure-analyzer agent.

**Inputs:**
- Test failure output (from test runner)
- Implementation code (files that failed tests)
- Validator context (why original changes were made)
- Generated test code

**Output expected:**
- For each failure: what test expected, what code did, what needs to be fixed
- Organized by failure category
- Implementation file references

Example:

```
ğŸ”´ TEST FAILURE: User should throw error for invalid email
   ğŸ“ src/domain/User.test.ts:52
   âŒ Expected: Constructor to throw "Invalid email"
   ğŸ“Š Got: Constructor succeeded, created user
   ğŸ’¡ Fix needed: Call validateEmail() in constructor
   ğŸ” Context: DDD validator introduced validation logic
   ğŸ“„ Implementation: src/domain/User.ts:5-12
```

#### 4j. Continue Loop or Terminate

Check termination conditions:

**Condition 1: All tests pass**
If all tests passed in 4i â†’ Success, go to Step 5 (Summary)

**Condition 2: Test failures with iterations remaining**
If tests failed AND iteration < maxIterations:
```
â†’ Re-running validators with test failure context...
```
Pass to validators:
- Test failure analysis from 4i
- Implementation code that failed tests
- Original validator context

Validators will receive NEW context: tests expect X, implementation does Y, fix needed.

Reset: `iteration++`, go back to 4b (Run Validators) but with failure context

**Condition 3: Max iterations with failures**
If iteration >= maxIterations AND tests still failing:
```
â¹ï¸ Max iterations reached with {n} failing tests.
```
â†’ Go to Step 5 (Summary) with warning

**Condition 4: Stall (no progress for 2 iterations)**
If same tests fail identically for 2 iterations:
```
âš ï¸ No test progress for 2 iterations. Stopping.
```
â†’ Go to Step 5 (Summary)

#### 4k. Continue Next Iteration

If continuing to next iteration:
- Update: `previousIssueCount` (for stall detection)
- Update: `iteration++`
- Mark todo completed, next marked as in_progress
- Go back to 4a (Iteration header)

#### 4l. Continue Loop

```
â†’ Continuing to iteration {n+1}...
```

Update:
- `previousIssueCount = currentIssueCount`
- `iteration++`
- Mark current todo as completed, next as in_progress

Go back to Step 4a.

### Step 5: Generate Final Summary

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 {âœ… Fix Loop Complete | âš ï¸ Fix Loop Stopped}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Iterations: {used}/{max}
Issues fixed: {totalFixed}
Remaining issues: {currentIssueCount}

{If test generation was enabled:}
Tests Generated: {count}
Tests Status: {âœ… All passing | âŒ {n} failing}

{If remaining issues > 0}
ğŸ“‹ Remaining issues require manual review:
{list remaining issues}

{If test generation enabled and tests failing}
âš ï¸ {n} tests still failing:
{list failing tests}
```

## Validator Agent Mapping

| Selection | Agent (subagent_type) |
|-----------|----------------------|
| DDD/OOP | ddd-oop-validator |
| DRY | dry-violations-detector |
| Clean Code | clean-code-validator |

## Important Notes

- Always run validators in **parallel** using multiple Task tool calls
- The fixer agent is: `fix-loop-fixer`
- Scope is always **branch diff** (files changed vs main or HEAD~5)
- Stop if no progress for 2 consecutive iterations (prevents infinite loops)
- Be transparent: show what's being fixed before and after
- Test generation is **optional** - can be disabled for faster iteration
- Test-driven fixing uses the same validators, just with new context (test failures)
- If tests fail consistently, likely indicates deeper issue requiring manual review
- Test generation agent will skip trivial code (getters, simple pass-throughs)
- Test output parsing tolerates some variation but warns on unparseable tests
